{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Workshop_Day2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVRNLQqIXbbpiCKI5MyVMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiernee/AI_Tutorial/blob/main/AI_Workshop_Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gWDkwnrM3x5"
      },
      "source": [
        "## Part 1: Boston Housing Price Prediction with Feed Forward Neural Networks\n",
        "\n",
        "Let's start with using a fully-connected neural network to do predict housing prices. \n",
        "\n",
        "For the Boston housing dataset, we get 506 rows of data, with 13 features in each. Our task is to build a regression model that takes these 13 features as input and output a single value prediction of the \"median value of owner-occupied homes (in $1000).\"\n",
        "\n",
        "Now, we load the dataset. Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gb0OqiINE4o"
      },
      "source": [
        "## 1. Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMkGmTRvOOrg"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "\n",
        "#A bunch is you remember is a dictionary based dataset.  Dictionaries are addressed by keys. \n",
        "#Let's look at the keys. \n",
        "print(boston.keys())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF-rxWBkOY4B"
      },
      "source": [
        "#DESCR sounds like it could be useful. Let's print the description.\n",
        "print(boston['DESCR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vlsu870OoAO"
      },
      "source": [
        "# 2. Data Splitting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjWCedhzzL8g"
      },
      "source": [
        "from tensorflow import keras\n",
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2iBZH4gOvlo"
      },
      "source": [
        "### **Exercise**\n",
        "\n",
        "How many number of samples in train and test datasets respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE5mrKUSOw9N"
      },
      "source": [
        "# type your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "km53WSDnO3GF"
      },
      "source": [
        "#@title Solution\n",
        "print('train_features: ', len(train_features))\n",
        "print('test_features: ', len(test_features))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxXXRpdgOrLv"
      },
      "source": [
        "### 3. Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model. First we stack a few layers together using `keras.Sequential`. Next we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
        "\n",
        "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
        "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
        "* *Metrics* - used to monitor the training and testing steps.\n",
        "\n",
        "Let's build a network with 1 hidden layer of 20 neurons, and use mean squared error (MSE) as the loss function (most common one for regression problems):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuYVRuKCPLUQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.optimizers.Adam(), \n",
        "                  loss='mse',\n",
        "                  metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL0O4x2XR2wz"
      },
      "source": [
        "### 4. Train the model\n",
        "\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model—in this example, the `train_features` and `train_labels` arrays.\n",
        "2. The model learns to associate features and labels.\n",
        "3. We ask the model to make predictions about a test set—in this example, the `test_features` array. We verify that the predictions match the labels from the `test_labels` array. \n",
        "\n",
        "To start training,  call the `model.fit` method—the model is \"fit\" to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXOf96rPSR7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# this helps makes our output less verbose but still shows progress\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0: print('')\n",
        "        print('.', end='')\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "history = model.fit(train_features, train_labels, epochs=1000, verbose=0, validation_split = 0.1,\n",
        "                    callbacks=[early_stop, PrintDot()])\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "# show RMSE measure to compare to Kaggle leaderboard on https://www.kaggle.com/c/boston-housing/leaderboard\n",
        "rmse_final = np.sqrt(float(hist['val_mse'].tail(1)))\n",
        "print()\n",
        "print('Final Root Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAGfF-hwPVKA"
      },
      "source": [
        "Now, let's plot the loss function measure on the training and validation sets. The validation set is used to prevent overfitting ([learn more about it here](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)). However, because our network is small, the training convergence without noticeably overfitting the data as the plot shows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvaxFwclPXUL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error [Thousand Dollars$^2$]')\n",
        "    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,100])\n",
        "\n",
        "plot_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ICQecGPY-G"
      },
      "source": [
        "## 5. Evaluate accuracy\n",
        "\n",
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MJjHzCoPfY8"
      },
      "source": [
        "_, mse = model.evaluate(test_features, test_labels)\n",
        "rmse = np.sqrt(mse)\n",
        "print('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s98YGXOoPhmr"
      },
      "source": [
        "# **Part 2. Design simple ANN model to predict SGA baby**\n",
        "## Classification of Normal vs. Small baby\n",
        "\n",
        "Task: Create a ANN model to predict the normal / small baby based on the second trimesters data. \n",
        "\n",
        "**Features:**\n",
        "1. Age\n",
        "2. Ethnics\n",
        "3. Head Circumference\n",
        "4. Abdominal Circumference \n",
        "2. Femur \n",
        "\n",
        "**Label:**\n",
        "1. Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w_MAhgTEOhA"
      },
      "source": [
        "# type your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEqVqTRpEChM"
      },
      "source": [
        "# Solutions\n",
        "Please try to do it yourself first. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8cImmAsut4l"
      },
      "source": [
        "### Step1: Import data\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4K4QZGluzTo"
      },
      "source": [
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/shiernee/AI_Tutorial/main/fetal_dataset.csv'\n",
        "data = pd.read_csv(url, delimiter=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vlDd1rezCBv"
      },
      "source": [
        "X = data[['Age', 'Ethnics', 'HC', 'AC', 'FL']]\n",
        "y = data['labels (0-healthy, 1-SGA)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WPGE74dCFE6"
      },
      "source": [
        "### Normalize your data (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUDCyl3x6cyR"
      },
      "source": [
        "import numpy as np\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "X_mean = np.mean(X, axis=0)\n",
        "X_std = np.std(X, axis=0)\n",
        "X_features = (X - X_mean) / X_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb0cLPICzN6F"
      },
      "source": [
        "## Step2: Data Splitting. \n",
        "Refer to this. <br> \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6MrXCUQzf7K"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X_features, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6VyduF6zy5j"
      },
      "source": [
        "## Step3: Build Model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wqZy-Ioz7Mz"
      },
      "source": [
        "input_dim = int(train_features.size/train_features.shape[0]) \n",
        "\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[input_dim]),\n",
        "        # output a softmax to squash the matrix into output probabilities\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.optimizers.Adam(), \n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdqhVCTo0Zu7"
      },
      "source": [
        "## Step4: Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9FeGSNa0cse"
      },
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "history = model.fit(train_features, train_labels, epochs=1000, verbose=0, validation_split = 0.1,\n",
        "                    callbacks=[early_stop, PrintDot()])\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "\n",
        "# show accuracy measure \n",
        "train_acc = float(hist['accuracy'].tail(1))\n",
        "val_acc = float(hist['val_accuracy'].tail(1))\n",
        "print()\n",
        "print('Train accuracy:', train_acc)\n",
        "print('Validation accuracy:', val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRYsF5O_6yXV"
      },
      "source": [
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error [Thousand Dollars$^2$]')\n",
        "    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,1])\n",
        "\n",
        "plot_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhMTgLjI7mD-"
      },
      "source": [
        "As the model trains, the loss and accuracy metrics are displayed. This model reaches an training accuracy of about 71% and validation accuracy 62% on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NiaJ5e77Ga5"
      },
      "source": [
        "## 5. Evaluate accuracy\n",
        "\n",
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdwLIR_J7NmD"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_features, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRyrhR188Ynw"
      },
      "source": [
        "Often times, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of *overfitting*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHaWWAhO8e1F"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "The contents of the Part 1: Regression tutorial is inspired and based on Lex Friedman's [tutorial_deep_learning_basic.ipynb](https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb#scrollTo=IysPmcOBHBE9) \n"
      ]
    }
  ]
}